{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5cf61c4b",
      "metadata": {
        "id": "5cf61c4b"
      },
      "source": [
        "# Programming Task Description\n",
        "\n",
        "## Programming Task: Implementing a Character-Level GPT Model\n",
        "\n",
        "### Introduction\n",
        "In this task, you will create a Python script using PyTorch to implement a simplified GPT (Generative Pre-trained Transformer) model for character-level language modeling. The model will be trained on the text in input.txt to predict the next character in a sequence and generate new text based on a given context. The architecture follows the decoder part of the transformer model from the \"Attention is All You Need\" paper by Vaswani et al., focusing on masked multi-head self-attention to ensure predictions depend only on previous positions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8537c2f6",
      "metadata": {
        "id": "8537c2f6"
      },
      "source": [
        "## Task Description\n",
        "### Your goal is to write a Python jupyter notebook that:\n",
        "\n",
        "1. Reads and processes the text from input.txt.\n",
        "2. Implements a decoder-only transformer model with manual attention mechanisms.\n",
        "3. Trains the model on the processed data.\n",
        "4. Generates new text using the trained model.\n",
        "\n",
        "You will use PyTorch and implement the attention mechanism from scratch, following the decoder structure outlined in the \"Attention is All You Need\" paper."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df79368",
      "metadata": {
        "id": "2df79368"
      },
      "source": [
        "### Step-by-step Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5d36b1",
      "metadata": {
        "id": "1d5d36b1"
      },
      "source": [
        "1. Data Preparation\n",
        "* Read all text from input.txt using UTF-8 encoding.\n",
        "* Create a sorted list of unique characters (vocabulary) from the text.\n",
        "* Build two dictionaries:\n",
        "    * stoi: Maps characters to integers (e.g., 'a' -> 0).\n",
        "    * itos: Maps integers to characters (e.g., 0 -> 'a').\n",
        "* Define functions:\n",
        "    * encode(s): Converts a string to a list of integers using stoi.\n",
        "    * decode(l): Converts a list of integers to a string using itos.\n",
        "* Encode the entire text into a tensor of integers using torch.tensor.\n",
        "* Split the data: 90% for training, 10% for validation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37a3e46",
      "metadata": {
        "id": "d37a3e46"
      },
      "source": [
        "2. Data Loading\n",
        "* Implement a function get_batch(split):\n",
        "    * Input: split is either 'train' or 'val'.\n",
        "    * Select the appropriate dataset (training or validation).\n",
        "    * Randomly sample batch_size starting indices, ensuring each sequence fits within block_size.\n",
        "* Return:\n",
        "    * x: A tensor of shape (batch_size, block_size) with input sequences.\n",
        "    * y: A tensor of shape (batch_size, block_size) with target sequences (shifted by one position).\n",
        "* Move tensors to the device (CPU or GPU)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f100337",
      "metadata": {
        "id": "6f100337"
      },
      "source": [
        "3. Model Architecture\n",
        "* Implement the following components in a decoder-only transformer:\n",
        "    * Embedding Layers:\n",
        "        * Token embedding: nn.Embedding(vocab_size, n_embd) for character indices.\n",
        "        * Position embedding: nn.Embedding(block_size, n_embd) for positions 0 to block_size-1.\n",
        "    * Transformer Blocks:\n",
        "        * Each block includes:\n",
        "            * Masked Multi-Head Self-Attention:\n",
        "                * Implement manually (do not use nn.MultiheadAttention).\n",
        "                * For each head:\n",
        "                    * Linear layers for queries (Q), keys (K), and values (V).\n",
        "                    * Scaled dot-product attention: attention = softmax((Q @ K.T) / sqrt(d_k)) @ V.\n",
        "                    * Mask future positions with a lower triangular matrix (e.g., tril) by setting future weights to -inf before softmax.\n",
        "                * Concatenate heads and apply a projection layer.\n",
        "            * Feed-Forward Network: nn.Linear(n_embd, 4 * n_embd) → ReLU → nn.Linear(4 * n_embd, n_embd).\n",
        "            * Layer Normalization: Apply nn.LayerNorm(n_embd) before each sub-layer (pre-norm).\n",
        "            * Residual Connections: Add input to output of each sub-layer.\n",
        "        * Use n_layer blocks in sequence.\n",
        "    * Final Layers:\n",
        "        * nn.LayerNorm(n_embd) for final normalization.\n",
        "        * nn.Linear(n_embd, vocab_size) to produce logits.\n",
        "* Define a GPTLanguageModel class with:\n",
        "    * forward(idx, targets=None): Computes logits and loss (if targets provided).\n",
        "    * generate(idx, max_new_tokens): Autoregressively generates new tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2098832e",
      "metadata": {
        "id": "2098832e"
      },
      "source": [
        "4. Training\n",
        "* Use the AdamW optimizer with learning_rate = 3e-4.\n",
        "* Train for max_iters = 5000 iterations.\n",
        "* Estimate and print training and validation losses:\n",
        "* Compute loss using F.cross_entropy on flattened logits and targets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1838a7a8",
      "metadata": {
        "id": "1838a7a8"
      },
      "source": [
        "5. Text Generation\n",
        "* Implement generate(idx, max_new_tokens):\n",
        "    * Start with an initial context idx (shape (B, T)).\n",
        "    * For max_new_tokens steps:\n",
        "        * Crop idx to the last block_size tokens.\n",
        "        * Get logits from forward.\n",
        "        * Apply softmax to the last time step’s logits to get probabilities.\n",
        "        * Sample the next token using torch.multinomial.\n",
        "        * Append the sampled token to idx.\n",
        "    * Return the extended sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d2b8dab",
      "metadata": {
        "id": "0d2b8dab"
      },
      "source": [
        "### Hyperparameters\n",
        "Use these values:\n",
        "\n",
        "* batch_size = 64\n",
        "* block_size = 256\n",
        "* n_embd = 384\n",
        "* n_head = 6\n",
        "* n_layer = 6\n",
        "* dropout = 0.2\n",
        "* learning_rate = 3e-4\n",
        "* max_iters = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d1a954",
      "metadata": {
        "id": "89d1a954"
      },
      "source": [
        "### Understanding the Decoder\n",
        "The \"Attention is All You Need\" paper describes a transformer with an encoder and decoder. For this task, you focus on the decoder-only architecture used in GPT:\n",
        "\n",
        "* Masked Self-Attention: Ensures the model only attends to previous positions in the sequence, making it autoregressive. This is achieved by masking future tokens in the attention computation, as shown below:\n",
        "\n",
        "$Attention (Q, K, V) = softmax((Q@K.T)/sqrt(d_{k}) + mask) @V$\n",
        "\n",
        "where $mask$ sets future positions to $-inf$\n",
        "\n",
        "* Decoder Role: In the original paper, the decoder generates output sequences while attending to the encoder’s output. Here, without an encoder, it uses self-attention on the input sequence alone, predicting the next token step-by-step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b33caf5",
      "metadata": {
        "id": "5b33caf5"
      },
      "source": [
        "### Additional Notes\n",
        "* Manual Attention: Implement attention from scratch to understand its mechanics (no pre-built PyTorch modules).\n",
        "* Masking: Use a lower triangular matrix (e.g., torch.tril) to mask future positions.\n",
        "* Device Handling: Set device = 'cuda' if torch.cuda.is_available() else 'cpu' and move tensors/models accordingly.\n",
        "* Dropout: Apply nn.Dropout(dropout) in attention and feed-forward layers for regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0da406",
      "metadata": {
        "id": "9c0da406"
      },
      "source": [
        "### Deliverables\n",
        "A Python script that:\n",
        "* Implements all steps above.\n",
        "* Prints training and validation losses every 500/100? iterations (up to you).\n",
        "* Generates and prints a 500-character sample after training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b7b3ac",
      "metadata": {
        "id": "36b7b3ac"
      },
      "source": [
        "### Evaluation Criteria\n",
        "* Correct data preparation and batch loading.\n",
        "* Accurate implementation of the transformer model, especially masked self-attention.\n",
        "* Successful training with decreasing loss.\n",
        "* Generation of coherent (for character-level) text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6779e8b8",
      "metadata": {
        "id": "6779e8b8"
      },
      "source": [
        "### Step 1: Import libraries and prepare data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70f40b97",
      "metadata": {
        "id": "70f40b97"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adbec8c5",
      "metadata": {
        "id": "adbec8c5"
      },
      "source": [
        "### Step 2: Load and process text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e708cdca",
      "metadata": {
        "id": "e708cdca"
      },
      "outputs": [],
      "source": [
        "# open and read the input text\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# get sorted list of unique characters\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create mappings\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# encoding and decoding functions\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "# convert full dataset to tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# split into train and val\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76731fe7",
      "metadata": {
        "id": "76731fe7"
      },
      "source": [
        "### Step 3: Define hyperparameters and data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2847bf80",
      "metadata": {
        "id": "2847bf80"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "block_size = 256\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "\n",
        "# get a batch of input and target sequences\n",
        "def get_batch(split):\n",
        "    d = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(0, len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30ccf33",
      "metadata": {
        "id": "f30ccf33"
      },
      "source": [
        "### Step 4: Build model components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ffb0da5",
      "metadata": {
        "id": "9ffb0da5"
      },
      "outputs": [],
      "source": [
        "# one attention head\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super(Head, self).__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        # Compute scaled dot-product attention\n",
        "        weight = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
        "        weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # mask future positions\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "        weight = self.dropout(weight)\n",
        "        v = self.value(x)\n",
        "        out = weight @ v\n",
        "        return out # return weighted sum of values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zebJwiE2TSq2",
      "metadata": {
        "id": "zebJwiE2TSq2"
      },
      "source": [
        "Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Q4_Je1nOTROl",
      "metadata": {
        "id": "Q4_Je1nOTROl"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concatenate heads\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out # project back to embedding size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c5043e",
      "metadata": {
        "id": "e4c5043e"
      },
      "source": [
        "Feedforward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "96165887",
      "metadata": {
        "id": "96165887"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e93b10",
      "metadata": {
        "id": "f3e93b10"
      },
      "source": [
        "Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eb9289d2",
      "metadata": {
        "id": "eb9289d2"
      },
      "outputs": [],
      "source": [
        "# transformer block = attention + ff + residual + norm\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Block, self).__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # attention + residual\n",
        "        x = x + self.ffwd(self.ln2(x)) # feedforward + residual\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424ca1cc",
      "metadata": {
        "id": "424ca1cc"
      },
      "source": [
        "### Step 5: Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "47c826f1",
      "metadata": {
        "id": "47c826f1"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GPT, self).__init__()\n",
        "        # Embedding for tokens and positions\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # Stack of Transformer blocks\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # final linear layer for logits\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_emb(idx)  # (B, T, n_embd)\n",
        "        pos_emb = self.pos_emb(torch.arange(T, device=device))  # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # add token + position embeddings\n",
        "        x = self.blocks(x) # apply Transformer blocks\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        # Compute cross-entropy loss if targets are provided\n",
        "        if targets is None:\n",
        "            return logits, None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            return logits, loss\n",
        "\n",
        "    # text generation\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:] # keep only the latest block_size tokens\n",
        "            logits, _ = self(idx_cond)  # forward pass\n",
        "            logits = logits[:, -1, :] # focus on last token only\n",
        "            probs = F.softmax(logits, dim=-1) # convert to probabilities\n",
        "            next_token = torch.multinomial(probs, num_samples=1) # sample next toke\n",
        "            idx = torch.cat((idx, next_token), dim=1) # append to sequence\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8c5a5d",
      "metadata": {
        "id": "1e8c5a5d"
      },
      "source": [
        "### Step 6: Create model and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1ff635aa",
      "metadata": {
        "id": "1ff635aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abeaef3-1b2e-4516-c583-faf6cfe69ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0:\n",
            "   Train Loss = 4.3798\n",
            "   Val Loss   = 3.6481\n",
            "Step 500:\n",
            "   Train Loss = 2.0748\n",
            "   Val Loss   = 2.0668\n",
            "Step 1000:\n",
            "   Train Loss = 1.6997\n",
            "   Val Loss   = 1.7245\n",
            "Step 1500:\n",
            "   Train Loss = 1.5302\n",
            "   Val Loss   = 1.6403\n",
            "Step 2000:\n",
            "   Train Loss = 1.4490\n",
            "   Val Loss   = 1.6107\n",
            "Step 2500:\n",
            "   Train Loss = 1.3245\n",
            "   Val Loss   = 1.5408\n",
            "Step 3000:\n",
            "   Train Loss = 1.2918\n",
            "   Val Loss   = 1.5076\n",
            "Step 3500:\n",
            "   Train Loss = 1.2876\n",
            "   Val Loss   = 1.4755\n",
            "Step 4000:\n",
            "   Train Loss = 1.2428\n",
            "   Val Loss   = 1.5397\n",
            "Step 4500:\n",
            "   Train Loss = 1.2130\n",
            "   Val Loss   = 1.5460\n"
          ]
        }
      ],
      "source": [
        "# Create model and optimizer\n",
        "model = GPT().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for step in range(max_iters):\n",
        "    model.train()\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss every 500 steps and check validation\n",
        "    if step % 500 == 0:\n",
        "        print(f\"Step {step}:\")\n",
        "        print(f\"   Train Loss = {loss.item():.4f}\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            xb_val, yb_val = get_batch('val')\n",
        "            _, val_loss = model(xb_val, yb_val)\n",
        "            print(f\"   Val Loss   = {val_loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8f4e542",
      "metadata": {
        "id": "c8f4e542"
      },
      "source": [
        "### Step 7: Generate output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0f0b167e",
      "metadata": {
        "id": "0f0b167e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2050f7d-5555-4260-9189-0bd117e1fee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "LUCIO:\n",
            "Here's brother, either you are a poor man.\n",
            "\n",
            "LUCIO:\n",
            "A sea tormors; within him that can prove him be crown'd\n",
            "this counted by the pettion of the purposition,\n",
            "A sin chief what obtain has accurse;\n",
            "And Besubashamas, are they like a mand off.\n",
            "\n",
            "bERBETH:\n",
            "The grave saves on him.\n",
            "\n",
            "RICHARD:\n",
            "Suspeiant one that time was to me too,\n",
            "That I am beat an intenedly fuce.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "O, like these tears, as though to noble.\n",
            "So Kill hate I had mark\n",
            "Afrown: what, the clouds? his mother own own of the lives\n"
          ]
        }
      ],
      "source": [
        "# Start with an empty context (just one token)\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Generate 500 characters of text\n",
        "output = model.generate(context, max_new_tokens=500)\n",
        "\n",
        "# Decode and print the generated character-level text\n",
        "print(decode(output[0].tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}